"""
Core/base_engine.py - á‹¨áˆ¶áˆµá‰± áˆµáˆªá‰¶á‰½ áˆ›á‹•áŠ¨áˆ‹á‹Š áˆá‰°áˆ­ (Universal Engine)
áˆˆ V9 (á‰€áˆ‹áˆ áˆµáˆ«)á£ v10 (áŠ¦á‰¶áˆœáˆ½áŠ•) áŠ¥áŠ“ v11 (áˆ‹á‰€ á‰¢á‹áŠáˆµ á‰µáŠ•á‰°áŠ“) á‹¨áˆšáˆ°áˆ«
"""

import os
import sys
import requests
import logging
import json
from urllib.parse import quote
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Union, Any
import hashlib
import time

# Logging setup for tracking across all versions
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(name)s] - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('profit_engine.log'),
        logging.StreamHandler()
    ]
)

class BaseProfitEngine:
    """áˆ¶áˆµá‰±áŠ•áˆ áˆµáˆªá‰¶á‰½ (v9, v10, v11) á‹¨áˆšá‹«áˆµá‰°áˆ³áˆµáˆ­ áˆ›á‹•áŠ¨áˆ‹á‹Š áˆá‰°áˆ­"""
    
    # Version-specific constants
    VERSION_CONFIG = {
        'v9': {
            'mode': 'standard',
            'research_level': 'basic',
            'content_type': 'article',
            'image_style': 'simple',
            'max_articles': 3,
            'timeout': 15
        },
        'v10': {
            'mode': 'enhanced',
            'research_level': 'intermediate',
            'content_type': 'enhanced_article',
            'image_style': 'infographic',
            'max_articles': 5,
            'timeout': 20
        },
        'v11': {
            'mode': 'enterprise',
            'research_level': 'advanced',
            'content_type': 'business_strategy',
            'image_style': 'professional',
            'max_articles': 8,
            'timeout': 30
        }
    }
    
    def __init__(self, version: str = 'v9', config_path: str = 'master_config.json'):
        """
        áˆ›á‹•áŠ¨áˆ‹á‹Š áˆá‰°áˆ­ áˆ˜áŒ€áˆ˜áˆªá‹« áŠ á‹°áˆ¨áŒƒáŒ€á‰µ
        
        Args:
            version (str): á‹¨áˆšáŒ á‰€áˆá‰ á‰µ áˆµáˆªá‰µ (v9, v10, v11)
            config_path (str): á‹¨á‰…áŠ•á‰¥áˆ­ á‹á‹­áˆ áˆ˜áŠ•áŒˆá‹µ
        """
        self.version = self._validate_version(version)
        self.logger = logging.getLogger(f"ProfitEngine_{self.version.upper()}")
        
        # Load configuration
        self.config = self._load_config(config_path)
        
        # Load API keys
        self._load_api_keys()
        
        # Set version-specific configuration
        self.version_config = self.VERSION_CONFIG.get(self.version, self.VERSION_CONFIG['v9'])
        
        # API Endpoints
        self.groq_url = "https://api.groq.com/openai/v1/chat/completions"
        self.news_url = "https://newsapi.org/v2/everything"
        
        # Cache for API responses
        self.cache = {}
        
        # Statistics tracking
        self.stats = {
            'api_calls': 0,
            'articles_fetched': 0,
            'content_generated': 0,
            'images_created': 0,
            'errors': 0
        }
        
        self.logger.info(f"ğŸš€ {self.version.upper()} áˆá‰°áˆ­ á‰°áŒ€áˆáˆ¯áˆ")
    
    def _validate_version(self, version: str) -> str:
        """á‹¨áˆµáˆªá‰µ áˆµáˆáŠ• á‹«áˆ¨áŒ‹áŒáŒ£áˆ"""
        valid_versions = ['v9', 'v10', 'v11']
        version_lower = version.lower()
        
        if version_lower not in valid_versions:
            self.logger.warning(f"á‹«áˆá‰°áˆ¨áŒ‹áŒˆáŒ  áˆµáˆªá‰µ: {version}. á‹¨áˆšáˆá‰€á‹°á‹ v9, v10, v11 á‰¥á‰» áŠá‹")
            return 'v9'
        
        return version_lower
    
    def _load_config(self, path: str) -> Dict:
        """áŠ¨ master_config.json áˆ˜áˆ¨áŒƒá‹á‰½áŠ• á‹«áŠá‰£áˆ"""
        try:
            if os.path.exists(path):
                with open(path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                self.logger.info(f"âœ… á‰…áŠ•á‰¥áˆ­ á‰°áŒ­áŠ—áˆ áŠ¨: {path}")
                
                # Merge version-specific config
                if self.version in config.get('version_overrides', {}):
                    config.update(config['version_overrides'][self.version])
                
                return config
            else:
                self.logger.warning(f"á‰…áŠ•á‰¥áˆ­ á‹á‹­áˆ áŠ áˆá‰°áŒˆáŠ˜áˆ: {path}")
                return self._create_default_config()
        except Exception as e:
            self.logger.error(f"Config áˆ›áˆáŒ£á‰µ áŠ áˆá‰°á‰»áˆˆáˆ: {e}")
            return self._create_default_config()
    
    def _create_default_config(self) -> Dict:
        """áˆ˜áˆ°áˆ¨á‰³á‹Š á‰…áŠ•á‰¥áˆ­ á‹­áˆáŒ¥áˆ«áˆ"""
        return {
            'api_settings': {
                'max_retries': 3,
                'cache_duration': 3600,  # 1 hour
                'rate_limit_delay': 1
            },
            'content_settings': {
                'max_length': 2000,
                'min_length': 300,
                'language': 'amharic'
            },
            'version_overrides': {
                'v9': {'simple_mode': True},
                'v10': {'automation': True},
                'v11': {'enterprise_features': True}
            }
        }
    
    def _load_api_keys(self):
        """áŠ¨áŠ áŠ«á‰£á‰¢ á‰°áˆˆá‹‹á‹‹áŒ®á‰½ áŠ¤á’áŠ á‹­ á‰áˆáá‰½áŠ• á‹«áŠá‰£áˆ"""
        self.groq_key = os.getenv('GROQ_API_KEY')
        self.news_key = os.getenv('NEWS_API_KEY')
        self.serper_key = os.getenv('SERPER_API_KEY')  # áˆˆ v10/v11 á‰°áŒ¨áˆ›áˆª á‹³áˆ°áˆ³
        
        # Validate API keys
        if not self.groq_key:
            self.logger.warning("GROQ_API_KEY áŠ áˆá‰°áŒˆáŠ˜áˆ. á‹¨áŠ á‹­ áŠ á‹­ á‹­á‹˜á‰µ áˆ›áˆ˜áŠ•áŒ¨á‰µ áŠ á‹­áˆ°áˆ«áˆ")
        
        if not self.news_key:
            self.logger.warning("NEWS_API_KEY áŠ áˆá‰°áŒˆáŠ˜áˆ. á‹¨á‹œáŠ“ á‹³áˆ°áˆ³ áŠ á‹­áˆ°áˆ«áˆ")
    
    def _get_cache_key(self, func_name: str, *args) -> str:
        """áˆˆáŠ«áˆ½ á‹¨á‰°áˆˆá‹¨ á‰áˆá á‹­áˆáŒ¥áˆ«áˆ"""
        key_string = f"{func_name}_{self.version}_{'_'.join(str(arg) for arg in args)}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _check_cache(self, cache_key: str) -> Optional[Any]:
        """áŠ«áˆ½ á‹áˆµáŒ¥ á‹«áˆˆá‹áŠ• á‹áŒ¤á‰µ á‹«á‹ˆáŒ£áˆ"""
        if cache_key in self.cache:
            cache_entry = self.cache[cache_key]
            if datetime.now() < cache_entry['expires']:
                self.logger.debug(f"áŠ«áˆ½ á‹áŒ¤á‰µ á‰°áŒˆáŠ˜: {cache_key}")
                return cache_entry['data']
            else:
                # á‹«áˆˆáˆ áŠ«áˆ½ áˆ›áŒ¥á‹á‰µ
                del self.cache[cache_key]
        return None
    
    def _save_to_cache(self, cache_key: str, data: Any, duration: int = 3600):
        """á‹áŒ¤á‰µáŠ• á‰ áŠ«áˆ½ á‹áˆµáŒ¥ á‹«áˆµá‰€áˆáŒ£áˆ"""
        expires = datetime.now() + timedelta(seconds=duration)
        self.cache[cache_key] = {
            'data': data,
            'expires': expires,
            'created': datetime.now()
        }
        self.logger.debug(f"á‹áŒ¤á‰µ á‰ áŠ«áˆ½ á‰°á‰€áˆáŒ§áˆ: {cache_key}")
    
    def fetch_research_data(self, topic: str, country: str = 'US') -> Dict[str, List]:
        """
        á‹¨á‰°á‹‹áˆƒá‹° á‹³áˆ°áˆ³ á‹˜á‹´ - áˆˆáˆ¶áˆµá‰±áˆ áˆµáˆªá‰¶á‰½
        
        Args:
            topic (str): á‹¨áˆšáˆáˆˆáŒˆá‹ áˆ­á‹•áˆ° áŒ‰á‹³á‹­
            country (str): áˆ€áŒˆáˆ­ (áˆáˆ­áŒ«)
            
        Returns:
            Dict: á‹¨á‹³áˆ°áˆ³ á‹áŒ¤á‰¶á‰½ á‰ á‰°áˆˆá‹«á‹© áŠ­ááˆá‰½
        """
        cache_key = self._get_cache_key('research', topic, country)
        cached_result = self._check_cache(cache_key)
        
        if cached_result:
            return cached_result
        
        self.logger.info(f"ğŸ” áˆáˆ­áˆáˆ­ áŠ¥á‹¨á‰°áŠ«áˆ„á‹° áŠá‹: {topic} in {country}")
        self.stats['api_calls'] += 1
        
        research_data = {
            'news': [],
            'market_data': [],
            'trends': [],
            'statistics': {}
        }
        
        # 1. News API Research (áˆˆáˆ¶áˆµá‰±áˆ áˆµáˆªá‰¶á‰½)
        if self.news_key:
            try:
                url = f"{self.news_url}?q={quote(topic)}&apiKey={self.news_key}&pageSize=10"
                response = requests.get(url, timeout=self.version_config['timeout'])
                
                if response.status_code == 200:
                    articles = response.json().get('articles', [])
                    max_articles = self.version_config['max_articles']
                    
                    for article in articles[:max_articles]:
                        research_data['news'].append({
                            'title': article.get('title', 'No title'),
                            'source': article.get('source', {}).get('name', 'Unknown'),
                            'description': article.get('description', '')[:200],
                            'url': article.get('url', ''),
                            'date': article.get('publishedAt', ''),
                            'relevance_score': self._calculate_relevance(topic, article.get('title', ''))
                        })
                    
                    self.stats['articles_fetched'] += len(research_data['news'])
                    self.logger.info(f"ğŸ“° {len(research_data['news'])} á‹œáŠ“á‹á‰½ á‰°áŒˆáŠá‰°á‹‹áˆ")
                else:
                    self.logger.warning(f"á‹¨á‹œáŠ“ áŠ¤á’áŠ á‹­ áˆµáˆ…á‰°á‰µ: {response.status_code}")
                    
            except Exception as e:
                self.logger.error(f"Research error: {e}")
                self.stats['errors'] += 1
        
        # 2. Additional market research for v10 and v11
        if self.version in ['v10', 'v11']:
            research_data.update(self._fetch_market_data(topic, country))
        
        # 3. Version-specific enhancements
        if self.version == 'v11':
            research_data['statistics'] = self._generate_statistics(research_data)
        
        # Cache the results
        self._save_to_cache(cache_key, research_data, duration=7200)  # 2 hours
        
        return research_data
    
    def _fetch_market_data(self, topic: str, country: str) -> Dict:
        """á‹¨áŒˆá‰ á‹« áˆ˜áˆ¨áŒƒ á‹«áŒˆáŠ›áˆ (áˆˆ v10/v11)"""
        market_data = {
            'market_size': 'á‰ áŒáˆá‰µ',
            'growth_rate': 'á‰ áŒáˆá‰µ',
            'competitors': [],
            'opportunities': []
        }
        
        # á‹­áˆ…áŠ• áŠ­ááˆ á‰ áŠ¥á‹áŠá‰°áŠ› á‹¨áŒˆá‰ á‹« á‹³á‰³ áŠ¤á’áŠ á‹­ áˆ˜áˆ™áˆ‹á‰µ á‹­á‰»áˆ‹áˆ
        if self.serper_key:
            try:
                # Serper API for market data (example)
                serper_url = "https://google.serper.dev/search"
                headers = {'X-API-KEY': self.serper_key}
                payload = {
                    "q": f"{topic} market size {country} 2024",
                    "num": 5
                }
                
                response = requests.post(serper_url, json=payload, headers=headers, timeout=15)
                if response.status_code == 200:
                    data = response.json()
                    # Process market data here
                    pass
                    
            except Exception as e:
                self.logger.debug(f"Market data fetch failed: {e}")
        
        return market_data
    
    def _calculate_relevance(self, topic: str, text: str) -> float:
        """á‹¨áŒ½áˆá áŒáŠ•áŠ™áŠá‰µ á‹°áˆ¨áŒƒ á‹«áˆ°áˆ‹áˆ"""
        topic_words = set(topic.lower().split())
        text_words = set(text.lower().split())
        
        if not topic_words or not text_words:
            return 0.0
        
        intersection = topic_words.intersection(text_words)
        return len(intersection) / len(topic_words)
    
    def _generate_statistics(self, research_data: Dict) -> Dict:
        """áˆµá‰³á‰²áˆµá‰²áŠ­áˆµ á‹­áˆáŒ¥áˆ«áˆ (áˆˆ v11)"""
        stats = {
            'total_sources': len(research_data.get('news', [])),
            'avg_relevance': 0,
            'date_range': '',
            'source_diversity': 0
        }
        
        if research_data.get('news'):
            relevance_scores = [item.get('relevance_score', 0) for item in research_data['news']]
            stats['avg_relevance'] = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0
            
            sources = set(item.get('source', '') for item in research_data['news'])
            stats['source_diversity'] = len(sources)
        
        return stats
    
    def generate_ai_content(self, topic: str, context_data: Dict, mode: str = None) -> str:
        """
        á‹¨áŠ á‹­ áŠ á‹­ á‹­á‹˜á‰µ á‹­áˆáŒ¥áˆ«áˆ - áˆˆáˆ¶áˆµá‰±áˆ áˆµáˆªá‰¶á‰½
        
        Args:
            topic (str): á‹¨áˆšáˆáˆˆáŒˆá‹ áˆ­á‹•áˆ° áŒ‰á‹³á‹­
            context_data (Dict): á‹¨á‹³áˆ°áˆ³ áˆ˜áˆ¨áŒƒ
            mode (str): á‹¨áˆ›áˆ˜áŠ•áŒ¨á‰µ áˆáŠá‰³ (standard/enterprise)
            
        Returns:
            str: á‹¨á‰°áˆ˜áŠáŒ¨á‹ á‹­á‹˜á‰µ
        """
        if not self.groq_key:
            return "áˆµáˆ…á‰°á‰µ: GROQ_API_KEY áŠ áˆá‰°áŒˆáŠ˜áˆ"
        
        # Determine mode based on version if not specified
        if mode is None:
            mode = 'enterprise' if self.version == 'v11' else 'standard'
        
        cache_key = self._get_cache_key('ai_content', topic, mode, str(context_data)[:100])
        cached_result = self._check_cache(cache_key)
        
        if cached_result:
            return cached_result
        
        self.logger.info(f"ğŸ¤– AI á‹­á‹˜á‰µ áŠ¥á‹¨á‰°áˆáŒ áˆ¨ áŠá‹ áˆˆ: {topic} (áˆáŠá‰³: {mode})")
        self.stats['api_calls'] += 1
        
        # Prepare prompts based on version and mode
        system_prompt, user_prompt = self._prepare_prompts(topic, context_data, mode)
        
        headers = {
            "Authorization": f"Bearer {self.groq_key}",
            "Content-Type": "application/json"
        }
        
        # Model selection based on version
        model_mapping = {
            'v9': 'llama3-8b-8192',
            'v10': 'mixtral-8x7b-32768',
            'v11': 'llama3-70b-8192'
        }
        
        payload = {
            "model": model_mapping.get(self.version, 'llama3-8b-8192'),
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0.7 if self.version == 'v9' else 0.5,
            "max_tokens": 1024 if self.version == 'v9' else 2048 if self.version == 'v10' else 4096
        }
        
        try:
            response = requests.post(self.groq_url, headers=headers, json=payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content']
                
                # Post-process content based on version
                content = self._post_process_content(content, mode)
                
                self.stats['content_generated'] += 1
                self.logger.info(f"âœ… AI á‹­á‹˜á‰µ á‰°áˆáŒ¥áˆ¯áˆ ({len(content)} á‰áˆáŠá‹á‰½)")
                
                # Cache the result
                self._save_to_cache(cache_key, content, duration=10800)  # 3 hours
                
                return content
            else:
                error_msg = f"API áˆµáˆ…á‰°á‰µ: {response.status_code} - {response.text}"
                self.logger.error(error_msg)
                self.stats['errors'] += 1
                return error_msg
                
        except Exception as e:
            error_msg = f"AI áˆ›áˆ˜áŠ•áŒ¨á‰µ áˆ‹á‹­ áˆµáˆ…á‰°á‰µ: {e}"
            self.logger.error(error_msg)
            self.stats['errors'] += 1
            return error_msg
    
    def _prepare_prompts(self, topic: str, context_data: Dict, mode: str) -> tuple:
        """áˆˆá‰°áˆˆá‹«á‹© áˆáŠá‰³á‹á‰½ á‹¨áˆšáˆ†áŠ• á•áˆ®áˆá•á‰µ á‹«á‹˜áŒ‹áŒƒáˆ"""
        
        if mode == 'enterprise':
            system_prompt = """áŠ áŠ•á‰° á‹¨áŠ¢áŠ•á‰°áˆ­á•áˆ«á‹­á‹ á‹°áˆ¨áŒƒ á‹¨á‰¢á‹áŠáˆµ áˆµá‰µáˆ«á‰´áŒ‚áˆµá‰µ áŠáˆ…á¢ 
            áˆˆáŠ¨áá‰°áŠ› áŠ áˆµá‰°á‹³á‹³áˆªá‹á‰½ á‹¨áˆšáˆ†áŠ• á‹áˆ­á‹áˆ­ á‹¨á‰¢á‹áŠáˆµ áˆµá‰µáˆ«á‰´áŒ‚ ááŒ áˆ­á¢
            á‹¨áˆšáŠ¨á‰°áˆ‰á‰µáŠ• áŠ áŠ«á‰µá:
            1. á‹¨áˆáŒ£áŠ• áˆ›áŒ á‰ƒáˆˆá‹«
            2. á‹¨áŒˆá‰ á‹« á‰µáŠ•á‰°áŠ“
            3. SWOT á‰µáŠ•á‰°áŠ“
            4. ROI á•áˆ®áŒ€áŠ­áˆ½áŠ•
            5. á‹¨áŒá‰¥á‹“á‰µ áŠ¥á‰…á‹µ"""
            
            user_prompt = f"""áˆˆ'{topic}' á‹¨áŠ¢áŠ•á‰°áˆ­á•áˆ«á‹­á‹ á‹°áˆ¨áŒƒ á‹¨á‰¢á‹áŠáˆµ áˆµá‰µáˆ«á‰´áŒ‚ ááŒ áˆ­á¢

á‹¨á‹³áˆ°áˆ³ áˆ˜áˆ¨áŒƒá‹á‰½:
{json.dumps(context_data, indent=2, ensure_ascii=False)[:2000]}

á‹¨á‰¢á‹áŠáˆµ áˆµá‰µáˆ«á‰´áŒ‚á‹ á‹áˆ­á‹áˆ­á£ á‰ á‹áˆ‚á‰¥ á‹¨á‰°á‹°áŒˆáˆ áŠ¥áŠ“ áˆˆáˆ˜á‰°áŒá‰ áˆ­ áŠ áŒá‰£á‰¥ á‹«áˆˆá‹ áˆ˜áˆ†áŠ• áŠ áˆˆá‰ á‰µá¢"""
        
        elif mode == 'enhanced':
            system_prompt = """áŠ áŠ•á‰° á‹¨á‰¢á‹áŠáˆµ á‰µáŠ•á‰°áŠ“ áˆŠá‰… áŠáˆ…á¢
            á‹¨á‰°áˆ»áˆ»áˆˆ á‹¨á‰¢á‹áŠáˆµ áŒ½áˆá ááŒ áˆ­ áŠ¨áŒ¥áˆá‰€á‰µ á‹«áˆˆá‹ á‰µáŠ•á‰°áŠ“ áŒ‹áˆ­á¢
            áŠ áˆµáˆáˆ‹áŒŠ á‹¨á‰¢á‹áŠáˆµ áˆƒáˆ³á‰¦á‰½áŠ• áŠ áŠ«á‰µáá¢"""
            
            user_prompt = f"""áˆµáˆˆ '{topic}' á‹áˆ­á‹áˆ­ á‹¨á‰¢á‹áŠáˆµ á‰µáŠ•á‰°áŠ“ áŒ½áˆá ááŒ áˆ­á¢

á‹¨á‹³áˆ°áˆ³ áˆ˜áˆ¨áŒƒ:
{json.dumps(context_data.get('news', []), indent=2, ensure_ascii=False)[:1500]}

áŒ½áˆá‰ áˆˆáŠ•áŒá‹µ áˆ°á‹á‰½ áŠ áŒˆáˆáŒáˆá‰µ á‹¨áˆšá‹«á‰€áˆ­á‰¥ áŠ¥áŠ“ áŠ áŒá‰£á‰¥ á‹«áˆ‰ áˆƒáˆ³á‰¦á‰½áŠ• áˆ˜á‹«á‹ áŠ áˆˆá‰ á‰µá¢"""
        
        else:  # standard mode
            system_prompt = """áŠ áŠ•á‰° á‰¥áˆ©áˆ… áŠ¥áŠ“ áˆ›áŠ•á‰ á‰¥ á‰€áˆ‹áˆ á‹¨áˆ†áŠ‘ áŒ½áˆáá‰½áŠ• á‹¨áˆá‰µáŒ½á á‹¨á‹œáŠ“ áŒ¸áˆáŠ áŠáˆ…á¢
            á‰ áŠ á‹²áˆµ áŠ á‰ á‰£ áˆ‹á‹­ á‹«áˆˆ á‹¨á‰¢á‹áŠáˆµ áˆ°á‹ áˆˆáˆšá‹«áŠá‰¥ áŠ á‹­áŠá‰µ áŒáˆáŒ½ áŠ¥áŠ“ áŠ áˆµá‰°áˆ›áˆª áŒ½áˆáá‰½áŠ• ááŒ áˆ­á¢"""
            
            user_prompt = f"""áˆµáˆˆ '{topic}' á‰€áˆ‹áˆ áŠ¥áŠ“ áˆˆáˆáˆ‰áˆ á‹¨áˆšá‰³á‹ˆá‰… áŒ½áˆá ááŒ áˆ­á¢

á‹¨á‹œáŠ“ áˆ˜áˆ¨áŒƒ:
{json.dumps([{'title': item.get('title', ''), 'source': item.get('source', '')} 
             for item in context_data.get('news', [])[:3]], indent=2, ensure_ascii=False)}

áŒ½áˆá‰ áŠ áŒ­áˆ­á£ áŒáˆáŒ½ áŠ¥áŠ“ áŠ áˆµá‹°áˆ³á‰½ áˆ˜áˆ†áŠ• áŠ áˆˆá‰ á‰µá¢"""
        
        return system_prompt, user_prompt
    
    def _post_process_content(self, content: str, mode: str) -> str:
        """á‹¨á‰°áˆ˜áŠáŒ¨á‹áŠ• á‹­á‹˜á‰µ á‰ áˆµáˆªá‰µ áˆ˜áˆ áˆ¨á‰µ á‹«áˆµá‰°áŠ«áŠ­áˆ‹áˆ"""
        
        # Add headers based on version
        if mode == 'enterprise':
            header = f"# á‹¨á‰¢á‹áŠáˆµ áˆµá‰µáˆ«á‰´áŒ‚ áˆ°áŠá‹µ\n## á‰€áŠ•: {datetime.now().strftime('%Y-%m-%d')}\n\n"
            footer = "\n\n---\n*á‹­áˆ… áˆ°áŠá‹µ á‰  ProfitEngine V11 á‰°áˆáŒ¥áˆ¯áˆ*"
            content = header + content + footer
            
        elif mode == 'enhanced':
            header = f"## á‹¨á‰¢á‹áŠáˆµ á‰µáŠ•á‰°áŠ“: {datetime.now().strftime('%B %d, %Y')}\n\n"
            content = header + content
            
        # Add Amharic formatting if needed
        if self.config.get('content_settings', {}).get('language') == 'amharic':
            # Ensure proper Amharic formatting
            content = content.replace('?', 'á§').replace('!', 'á¥')
        
        return content
    
    def generate_image_url(self, topic: str, style: str = None) -> str:
        """
        áˆáˆµáˆ á‹¨áˆšá‹«áˆ˜áŠáŒ­ áŠ áŒˆáˆáŒáˆá‰µ á‹­áŒ á‰…áˆ›áˆ
        
        Args:
            topic (str): á‹¨áˆšáˆáˆˆáŒˆá‹ áˆ­á‹•áˆ° áŒ‰á‹³á‹­
            style (str): á‹¨áˆáˆµáˆ á‹˜á‹­á‰¤ (áˆáˆ­áŒ«)
            
        Returns:
            str: á‹¨áˆáˆµáˆ‰ URL
        """
        if style is None:
            style = self.version_config['image_style']
        
        self.logger.info(f"ğŸ¨ áˆáˆµáˆ áŠ¥á‹¨á‰°áˆáŒ áˆ¨ áŠá‹ áˆˆ: {topic} (á‹˜á‹­á‰¤: {style})")
        self.stats['images_created'] += 1
        
        # Prepare image prompt based on version and style
        image_prompts = {
            'simple': f"simple illustration of {topic}, clean, minimal",
            'infographic': f"business infographic about {topic}, data visualization, professional",
            'professional': f"enterprise business concept for {topic}, executive style, high quality"
        }
        
        prompt = image_prompts.get(style, topic)
        
        # Add version-specific enhancements
        if self.version == 'v11':
            prompt = f"professional business strategy diagram: {prompt}"
        
        # Generate unique seed
        seed = datetime.now().microsecond + hash(topic) % 1000000
        
        # Use multiple image service options for fallback
        image_services = [
            f"https://image.pollinations.ai/prompt/{quote(prompt)}?width=1200&height=800&nologo=true&seed={seed}",
            f"https://api.placeholder.ai/v1/image?text={quote(prompt[:50])}&width=1200&height=800",
            f"https://dummyimage.com/1200x800/3498db/ffffff&text={quote(prompt[:30])}"
        ]
        
        return image_services[0]  # Return primary service
    
    def save_output(self, filename: str, data: str, version_folder: str = None) -> Dict:
        """
        á‹áŒ¤á‰±áŠ• á‰ áˆšáˆ˜áˆˆáŠ¨á‰°á‹ ááˆá‹°áˆ­ á‹áˆµáŒ¥ á‹«áˆµá‰€áˆáŒ£áˆ
        
        Args:
            filename (str): á‹¨á‹á‹­áˆ‰ áˆµáˆ
            data (str): á‹¨áˆšá‰€áˆ˜áŒ  á‹áˆ‚á‰¥
            version_folder (str): á‹¨áˆµáˆªá‰± ááˆá‹°áˆ­ (áˆáˆ­áŒ«)
            
        Returns:
            Dict: á‹¨áˆ›áˆµá‰€áˆ˜áŒ¢á‹« á‹áŒ¤á‰µ
        """
        if version_folder is None:
            version_folder = self.version.upper()
        
        # Create version folder if it doesn't exist
        os.makedirs(version_folder, exist_ok=True)
        
        # Clean filename
        safe_filename = self._clean_filename(filename)
        
        # Add timestamp and extension
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        final_filename = f"{safe_filename}_{timestamp}.txt"
        
        path = os.path.join(version_folder, final_filename)
        
        try:
            with open(path, 'w', encoding='utf-8') as f:
                f.write(data)
            
            self.logger.info(f"âœ… á‹á‹­áˆ á‰°á‰€áˆáŒ§áˆ: {path}")
            
            return {
                'success': True,
                'path': path,
                'filename': final_filename,
                'size': len(data),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"á‹á‹­áˆ áˆ›áˆµá‰€áˆ˜áŒ¥ áŠ áˆá‰°á‰»áˆˆáˆ: {e}")
            self.stats['errors'] += 1
            
            return {
                'success': False,
                'error': str(e),
                'path': path
            }
    
    def save_json_output(self, filename: str, data: Dict, version_folder: str = None) -> Dict:
        """
        JSON á‹áˆ‚á‰¥áŠ• á‹«áˆµá‰€áˆáŒ£áˆ
        
        Args:
            filename (str): á‹¨á‹á‹­áˆ‰ áˆµáˆ
            data (Dict): á‹¨áˆšá‰€áˆ˜áŒ¥ JSON á‹áˆ‚á‰¥
            version_folder (str): á‹¨áˆµáˆªá‰± ááˆá‹°áˆ­ (áˆáˆ­áŒ«)
            
        Returns:
            Dict: á‹¨áˆ›áˆµá‰€áˆ˜áŒ¢á‹« á‹áŒ¤á‰µ
        """
        try:
            json_data = json.dumps(data, indent=2, ensure_ascii=False)
            return self.save_output(filename, json_data, version_folder)
        except Exception as e:
            self.logger.error(f"JSON áˆ›áˆµá‰€áˆ˜áŒ¥ áŠ áˆá‰°á‰»áˆˆáˆ: {e}")
            return {'success': False, 'error': str(e)}
    
    def _clean_filename(self, filename: str) -> str:
        """á‹á‹­áˆ áˆµáˆáŠ• áˆˆáˆ›áŒ½á‹³á‰µ á‹­áŒ á‰…áˆ›áˆ"""
        # Remove invalid characters
        invalid_chars = ['<', '>', ':', '"', '/', '\\', '|', '?', '*']
        for char in invalid_chars:
            filename = filename.replace(char, '_')
        
        # Trim and limit length
        filename = filename.strip()
        if len(filename) > 100:
            filename = filename[:100]
        
        return filename
    
    def run_complete_pipeline(self, topic: str, country: str = 'US') -> Dict:
        """
        áˆ™áˆ‰á‹áŠ• á‹¨áˆµáˆ« áˆ‚á‹°á‰µ á‹«áˆµáˆáŒ½áˆ›áˆ
        
        Args:
            topic (str): á‹¨áˆšá‰°áŠá‰µáŠá‹ áˆ­á‹•áˆ° áŒ‰á‹³á‹­
            country (str): áˆ€áŒˆáˆ­
            
        Returns:
            Dict: á‹¨áˆ™áˆ‰á‹ áˆ‚á‹°á‰µ á‹áŒ¤á‰µ
        """
        self.logger.info(f"ğŸš€ á‹¨áˆ™áˆ‰ á‹á‹­áˆ áˆ‚á‹°á‰µ áŒ€áˆ˜áˆ¨ áˆˆ: {topic}")
        
        start_time = time.time()
        
        result = {
            'version': self.version,
            'topic': topic,
            'country': country,
            'timestamp': datetime.now().isoformat(),
            'pipeline_steps': {},
            'outputs': {},
            'statistics': self.stats.copy()
        }
        
        try:
            # Step 1: Research
            result['pipeline_steps']['research'] = 'started'
            research_data = self.fetch_research_data(topic, country)
            result['pipeline_steps']['research'] = 'completed'
            result['research_summary'] = {
                'news_count': len(research_data.get('news', [])),
                'market_data': len(research_data.get('market_data', [])),
                'trends': len(research_data.get('trends', []))
            }
            
            # Step 2: Content Generation
            result['pipeline_steps']['content_generation'] = 'started'
            mode = 'enterprise' if self.version == 'v11' else 'enhanced' if self.version == 'v10' else 'standard'
            content = self.generate_ai_content(topic, research_data, mode)
            result['pipeline_steps']['content_generation'] = 'completed'
            result['outputs']['content'] = content[:500] + "..." if len(content) > 500 else content
            
            # Step 3: Image Generation
            result['pipeline_steps']['image_generation'] = 'started'
            image_url = self.generate_image_url(topic)
            result['pipeline_steps']['image_generation'] = 'completed'
            result['outputs']['image_url'] = image_url
            
            # Step 4: Save Content
            result['pipeline_steps']['saving'] = 'started'
            save_result = self.save_output(topic, content)
            result['pipeline_steps']['saving'] = 'completed'
            result['outputs']['saved_file'] = save_result
            
            # Step 5: Save Research Data (for v10 and v11)
            if self.version in ['v10', 'v11']:
                research_save = self.save_json_output(f"{topic}_research", research_data)
                result['outputs']['research_file'] = research_save
            
            # Step 6: Generate report for v11
            if self.version == 'v11':
                report = self._generate_comprehensive_report(topic, content, research_data, image_url)
                report_save = self.save_output(f"{topic}_comprehensive_report", report)
                result['outputs']['comprehensive_report'] = report_save
            
            result['pipeline_steps']['overall'] = 'completed'
            result['success'] = True
            
            elapsed_time = time.time() - start_time
            result['execution_time'] = f"{elapsed_time:.2f} áˆ°áŠ¨áŠ•á‹µ"
            
            self.logger.info(f"âœ… á‹á‹­áˆ áˆ‚á‹°á‰± á‰°áŒ áŠ“á‰‹áˆ á‰  {elapsed_time:.2f} áˆ°áŠ¨áŠ•á‹µ")
            
        except Exception as e:
            result['pipeline_steps']['overall'] = 'failed'
            result['success'] = False
            result['error'] = str(e)
            self.logger.error(f"âŒ á‹á‹­áˆ áˆ‚á‹°á‰± áŠ áˆá‰°áˆ³áŠ«áˆ: {e}")
        
        return result
    
    def _generate_comprehensive_report(self, topic: str, content: str, research_data: Dict, image_url: str) -> str:
        """áˆˆ v11 á‹áˆ­á‹áˆ­ áˆªá–áˆ­á‰µ á‹­áˆáŒ¥áˆ«áˆ"""
        report = f"""# á‹¨á‰¢á‹áŠáˆµ áˆµá‰µáˆ«á‰´áŒ‚ áˆªá–áˆ­á‰µ
## áˆ­á‹•áˆ° áŒ‰á‹³á‹­: {topic}
## á‹¨á‰°áˆáŒ áˆ¨á‰ á‰µ á‰€áŠ•: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
## áˆµáˆªá‰µ: {self.version.upper()}

---

## 1. á‹¨áŒ¥áŠ“á‰µ áˆ›áŒ á‰ƒáˆˆá‹«
- áŒ á‰…áˆ‹áˆ‹ á‹¨á‹œáŠ“ áˆáŠ•áŒ®á‰½: {len(research_data.get('news', []))}
- á‹¨áŒˆá‰ á‹« áˆ˜áˆ¨áŒƒá‹á‰½: {len(research_data.get('market_data', []))}
- á‹¨áŒáŠ•á‹›á‰¤ á‹°áˆ¨áŒƒ: {research_data.get('statistics', {}).get('avg_relevance', 0):.2f}

---

## 2. á‹¨á‰¢á‹áŠáˆµ áˆµá‰µáˆ«á‰´áŒ‚
{content}

---

## 3. á‹¨áˆáˆµáˆ áˆ›áŒ£á‰€áˆ»
![Business Strategy]({image_url})

---

## 4. á‹¨áˆµáˆ­á‹“á‰µ áˆµá‰³á‰²áˆµá‰²áŠ­áˆµ
- á‹¨áŠ¤á’áŠ á‹­ áŒ¥áˆªá‹á‰½: {self.stats['api_calls']}
- á‹¨á‰°áŒˆáŠ™ á‹œáŠ“á‹á‰½: {self.stats['articles_fetched']}
- á‹¨á‰°áˆáŒ áˆ© á‹¨á‹­á‹˜á‰µ á‰áˆáŠá‹á‰½: {self.stats['content_generated']}
- áˆµáˆ…á‰°á‰¶á‰½: {self.stats['errors']}

---

*á‹­áˆ… áˆªá–áˆ­á‰µ á‰  ProfitEngine V11 á‰°áˆáŒ¥áˆ¯áˆ*
"""
        return report
    
    def get_statistics(self) -> Dict:
        """á‹¨áŠ áˆáŠ‘áŠ• áˆµá‰³á‰²áˆµá‰²áŠ­áˆµ á‹«áˆ³á‹«áˆ"""
        return {
            'version': self.version,
            'timestamp': datetime.now().isoformat(),
            'statistics': self.stats,
            'cache_size': len(self.cache),
            'config_version': self.config.get('version', '1.0')
        }
    
    def clear_cache(self):
        """áŠ«áˆ½ á‹«áŒ½á‹³áˆ"""
        self.cache.clear()
        self.logger.info("âœ… áŠ«áˆ½ á‰°áŒ½á‹µá‰‹áˆ")

# Utility function for easy import
def create_engine(version: str = 'v9', config_path: str = 'master_config.json') -> BaseProfitEngine:
    """
    áˆá‰°áˆ­ áˆˆáˆ˜ááŒ áˆ­ á‰€áˆ‹áˆ á‰°áŒá‰£áˆ­
    
    Args:
        version (str): á‹¨áˆšáˆáˆáŒ‰á‰µ áˆµáˆªá‰µ
        config_path (str): á‹¨á‰…áŠ•á‰¥áˆ­ á‹á‹­áˆ áˆ˜áŠ•áŒˆá‹µ
        
    Returns:
        BaseProfitEngine: á‹¨á‰°áˆáŒ áˆ¨ áˆá‰°áˆ­
    """
    return BaseProfitEngine(version=version, config_path=config_path)

# Example usage
if __name__ == "__main__":
    # Test the engine
    engine = BaseProfitEngine(version='v11')
    
    # Run complete pipeline
    result = engine.run_complete_pipeline("á‹¨áŠ¢á‰µá‹®áŒµá‹« á‹¨á‰´áŠ­ áŠ¢áŠ•á‹±áˆµá‰µáˆª", country="ET")
    
    print(f"áˆµáˆªá‰µ: {result['version']}")
    print(f"áˆ­á‹•áˆ° áŒ‰á‹³á‹­: {result['topic']}")
    print(f"áˆáŠ”á‰³: {'âœ… á‰°áˆ³áŠ­á‰·áˆ' if result.get('success') else 'âŒ áŠ áˆá‰°áˆ³áŠ«áˆ'}")
    print(f"áˆ°á‹“á‰µ: {result.get('execution_time', 'N/A')}")
    print(f"á‹¨á‹œáŠ“á‹á‰½ á‰¥á‹›á‰µ: {result.get('research_summary', {}).get('news_count', 0)}")
    
    # Get statistics
    stats = engine.get_statistics()
    print(f"\nğŸ“Š áˆµá‰³á‰²áˆµá‰²áŠ­áˆµ:")
    print(f"  á‹¨áŠ¤á’áŠ á‹­ áŒ¥áˆªá‹á‰½: {stats['statistics']['api_calls']}")
    print(f"  á‹¨á‰°áˆáŒ áˆ© á‹­á‹˜á‰¶á‰½: {stats['statistics']['content_generated']}")
